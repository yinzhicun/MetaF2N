# MetaF2N: Blind Image Super-Resolution by Learning Efficient Model Adaptation from Faces

<a href="https://arxiv.org/pdf/2309.08113.pdf"><img src="https://img.shields.io/badge/arXiv-2309.08113-b31b1b.svg" height=22.5></a>

>**Abstract**: <br>
> Due to their highly structured characteristics, faces are easier to recover than natural scenes for blind image super-resolution. Therefore, we can extract the degradation representation of an image from the low-quality and recovered face pairs. Using the degradation representation, realistic low-quality images can then be synthesized to fine-tune the super-resolution model for the real-world low-quality image. However, such a procedure is time-consuming and laborious, and the gaps between recovered faces and the ground-truths further increase the optimization uncertainty. To facilitate efficient model adaptation towards image-specific degradations, we propose a method dubbed MetaF2N, which leverages the contained Faces to fine-tune model parameters for adapting to the whole Natural image in a Meta-learning framework. The degradation extraction and low-quality image synthesis steps are thus circumvented in our MetaF2N, and it requires only one fine-tuning step to get decent performance. Considering the gaps between the recovered faces and ground-truths, we further deploy a MaskNet for adaptively predicting loss weights at different positions to reduce the impact of low-confidence areas. To evaluate our proposed MetaF2N, we have collected a real-world low-quality dataset with one or multiple faces in each image, and our MetaF2N achieves superior performance on both synthetic and real-world datasets.
![method](assets/method.png)



## Getting Started

### Environment Setup

```shell
git clone https://github.com/yinzhicun/MetaF2N.git
cd MetaF2N
conda create -n metaf2n python=3.9
conda activate metaf2n
pip install -r requirements.txt
```

### Pretrained Models

We provide the pretrained checkpoints in [BaiduDisk](https://pan.baidu.com/s/1Up3W9OKVNgdZT1mwQHZeoQ?pwd=7fm2) and Google Drive(will be prepared soon). One can download them and save to the directory `./pretrained_models`.

Moreover, the other models (pretrained GPEN, Real-ESRGAN and RetinaFace) we need for training and testing are also provided in [BaiduDisk](https://pan.baidu.com/s/13GEXwwA2250V18_oyWuSnA?pwd=8es6) and Google Drive(will be prepared soon). One can download thme and save to the directory `./weights`.

### Preparing Dataset

- Since we use tensoflow as our framework, we prepare our training data in the format of .tfrecord as [BaiduDisk](https://pan.baidu.com/s/1jNOJEbFr1KdFT08Gfmanqw?pwd=9qgp) and Google Drive(will be prepared soon). One can download them and save to the directory `./datasets`.

- If you want to prepare the training data yourself, you can use the generate_tfrecord.py and change the parameters.
    ```shell
    python scripts/generate_tfrecord.py
    ```

- All the test data are provided as [BaiduDisk](https://pan.baidu.com/s/1jNOJEbFr1KdFT08Gfmanqw?pwd=9qgp) and Google Drive(will be prepared soon). For each synthsized dataset (FFHQ_iid, FFHQ_ood, CelebA_iid, CelebA_ood, FFHQ_Multi_iid, FFHQ_Multi_ood), it has four subfolders which are GT, LQ, Face_LQ and Face_HQ. And the RealFaces200 dataset do not have GT. Noted that HQ face images in Face_HQ are pregenerated by us, you can generate them youself with generate_test_faces.py.
    ```shell
    python generate_test_faces.py --input_dir input_dir --output_dir output_dir
    ```

### Testing

To test the method, you can run,

```Shell
CUDA_VISIBLE_DEVICES=0 test.py --input_dir input_dir --output_dir output_dir --face_dir face_dir
```

### Training

To train MetaF2N, you can adjust the parameters in config.py and run,

```Shell
python main.py --trial trial --step step --gpu gpu_id
```

## Citation

```
@article{yin2023metaf2n,
  title={MetaF2N: Blind Image Super-Resolution by Learning Efficient Model Adaptation from Faces},
  author={Yin, Zhicun and Liu, Ming and Li, Xiaoming and Yang, Hui and Xiao, Longan and Zuo, Wangmeng},
  journal={arXiv preprint arXiv:2309.08113},
  year={2023}
}
```

## Acknowledgements

This code is built on [MZSR](https://github.com/JWSoh/MZSR) and [GPEN](https://github.com/yangxy/GPEN). We thank the authors for sharing the codes.